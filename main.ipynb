{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241f6387",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo - 15 Épocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c90526",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import v2 as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"Versão do PyTorch: {torch.__version__}\")\n",
    "print(f\"Versão do Torchvision: {torchvision.__version__}\")\n",
    "\n",
    "# --- CARREGANDO O DATASET ---\n",
    "class ArthropodDetectionDataset(Dataset):\n",
    "    def __init__(self, image_paths, annotation_paths, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.image_paths = image_paths\n",
    "        self.annotation_paths = annotation_paths\n",
    "        self.class_map = {'Araneae': 1, 'Coleoptera': 2, 'Diptera': 3, 'Hemiptera': 4, 'Hymenoptera': 5, 'Lepidoptera': 6, 'Odonata': 7}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        annotation_path = self.annotation_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes, labels = self.parse_annotation(annotation_path)\n",
    "\n",
    "        if not boxes:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # Função que abre o arquivo JSON e extrai as coordenadas\n",
    "    def parse_annotation(self, path):\n",
    "        boxes, labels = [], []\n",
    "        try:\n",
    "            with open(path) as f: data = json.load(f)\n",
    "            for region in data['regions']:\n",
    "                class_name = region['tags'][0]\n",
    "                if class_name in self.class_map:\n",
    "                    label_id = self.class_map[class_name]\n",
    "                    bbox_data = region['boundingBox']\n",
    "                    xmin, ymin = bbox_data['left'], bbox_data['top']\n",
    "                    xmax, ymax = xmin + bbox_data['width'], ymin + bbox_data['height']\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(label_id)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return boxes, labels\n",
    "\n",
    "# Define o pré-processamento para o treino, usamos uma simples inversão horizontal\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToImage())\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# Definição da arquitetura usando um Faster R-CNN pré-treinado\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Função contém a lógica de uma única época de treinamento.\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch_num):\n",
    "    model.train()\n",
    "    print_freq = 50\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % print_freq == 0:\n",
    "            print(f\"Época [{epoch_num}] | Batch [{i+1}/{len(data_loader)}] | Loss: {losses.item():.4f}\")\n",
    "\n",
    "# --- PARÂMETROS PRINCIPAIS ---\n",
    "DATASET_PATH = '/kaggle/input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr'\n",
    "\n",
    "CHECKPOINT_DIR = '/kaggle/working/'\n",
    "ultimo_checkpoint_salvo = None\n",
    "num_epochs = 15\n",
    "batch_size = 4\n",
    "\n",
    "all_image_paths = []\n",
    "all_annotation_paths = []\n",
    "\n",
    "print(\"Mapeando todos os arquivos do dataset...\")\n",
    "for class_folder in sorted(os.listdir(DATASET_PATH)):\n",
    "    class_path = os.path.join(DATASET_PATH, class_folder)\n",
    "    if not os.path.isdir(class_path): continue\n",
    "    annotation_folder_path = os.path.join(class_path, 'annotations')\n",
    "    if not os.path.exists(annotation_folder_path): continue\n",
    "\n",
    "    annotation_map = {}\n",
    "    for json_file in os.listdir(annotation_folder_path):\n",
    "        json_path = os.path.join(annotation_folder_path, json_file)\n",
    "        try:\n",
    "            with open(json_path) as f: data = json.load(f)\n",
    "            asset_name = data['asset']['name']\n",
    "            annotation_map[asset_name] = json_path\n",
    "        except Exception: pass\n",
    "\n",
    "    for img_file in os.listdir(class_path):\n",
    "        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')) and img_file in annotation_map:\n",
    "            all_image_paths.append(os.path.join(class_path, img_file))\n",
    "            all_annotation_paths.append(annotation_map[img_file])\n",
    "\n",
    "print(f\"Mapeamento concluído! Total de {len(all_image_paths)} imagens encontradas.\")\n",
    "\n",
    "# Dividindo os caminhos (90% treino, 10% teste)\n",
    "paths = list(zip(all_image_paths, all_annotation_paths))\n",
    "train_paths, test_paths = train_test_split(paths, test_size=0.1, random_state=42)\n",
    "train_img_paths, train_ann_paths = zip(*train_paths)\n",
    "test_img_paths, test_ann_paths = zip(*test_paths)\n",
    "\n",
    "print(f\"Dados divididos em: {len(train_img_paths)} para treino e {len(test_img_paths)} para teste.\")\n",
    "\n",
    "# --- PREPARAÇÃO DOS DATASETS E DATALOADERS ---\n",
    "dataset_train = ArthropodDetectionDataset(\n",
    "    image_paths=list(train_img_paths),\n",
    "    annotation_paths=list(train_ann_paths),\n",
    "    transforms=get_transform(train=True)\n",
    ")\n",
    "dataset_test = ArthropodDetectionDataset(\n",
    "    image_paths=list(test_img_paths),\n",
    "    annotation_paths=list(test_ann_paths),\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Usando o dispositivo: {device}\")\n",
    "\n",
    "# --- CRIAÇÃO DO MODELO E OTIMIZADOR ---\n",
    "num_classes = 8\n",
    "model = get_model(num_classes).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# --- CARREGAMENTO DO CHECKPOINT ---\n",
    "start_epoch = 0\n",
    "if ultimo_checkpoint_salvo and os.path.exists(ultimo_checkpoint_salvo):\n",
    "    print(f\"Carregando checkpoint de '{ultimo_checkpoint_salvo}'...\")\n",
    "    checkpoint = torch.load(ultimo_checkpoint_salvo)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint carregado. Recomeçando o treinamento da época {start_epoch + 1}.\")\n",
    "else:\n",
    "    print(\"Nenhum checkpoint válido encontrado. Começando o treinamento do zero.\")\n",
    "\n",
    "# --- LOOP DE TREINAMENTO ---\n",
    "print(\"\\n--- INICIANDO O TREINAMENTO ---\")\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    epoch_num = epoch + 1\n",
    "    train_one_epoch(model, optimizer, data_loader_train, device, epoch_num)\n",
    "    print(f\"--- FIM DA ÉPOCA {epoch_num} ---\\n\")\n",
    "\n",
    "    checkpoint_save_path = os.path.join(CHECKPOINT_DIR, f'modelo_epoca_{epoch_num}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch_num,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_save_path)\n",
    "    print(f\"Checkpoint da época {epoch_num} salvo em: {checkpoint_save_path}\")\n",
    "\n",
    "print(\"--- TREINAMENTO CONCLUÍDO! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8601f4",
   "metadata": {},
   "source": [
    "## Teste do Modelo - Modelo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0e33b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import v2 as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import box_iou, nms\n",
    "\n",
    "print(f\"Versão do PyTorch: {torch.__version__}\")\n",
    "print(f\"Versão do Torchvision: {torchvision.__version__}\")\n",
    "\n",
    "class ArthropodDetectionDataset(Dataset):\n",
    "    def __init__(self, image_paths, annotation_paths, transforms):\n",
    "        self.transforms = transforms\n",
    "        self.image_paths = image_paths\n",
    "        self.annotation_paths = annotation_paths\n",
    "        self.class_map = {'Araneae': 1, 'Coleoptera': 2, 'Diptera': 3, 'Hemiptera': 4, 'Hymenoptera': 5, 'Lepidoptera': 6, 'Odonata': 7}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        annotation_path = self.annotation_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes, labels = self.parse_annotation(annotation_path)\n",
    "\n",
    "        if not boxes:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": torch.tensor([idx])}\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def parse_annotation(self, path):\n",
    "        boxes, labels = [], []\n",
    "        try:\n",
    "            with open(path) as f: data = json.load(f)\n",
    "            for region in data['regions']:\n",
    "                class_name = region['tags'][0]\n",
    "                if class_name in self.class_map:\n",
    "                    label_id = self.class_map[class_name]\n",
    "                    bbox_data = region['boundingBox']\n",
    "                    xmin, ymin = bbox_data['left'], bbox_data['top']\n",
    "                    xmax, ymax = xmin + bbox_data['width'], ymin + bbox_data['height']\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(label_id)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return boxes, labels\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToImage())\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "DATASET_PATH = '/kaggle/input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr'\n",
    "\n",
    "all_image_paths = []\n",
    "all_annotation_paths = []\n",
    "print(\"Mapeando todos os arquivos do dataset...\")\n",
    "for class_folder in sorted(os.listdir(DATASET_PATH)):\n",
    "    class_path = os.path.join(DATASET_PATH, class_folder)\n",
    "    if not os.path.isdir(class_path): continue\n",
    "    annotation_folder_path = os.path.join(class_path, 'annotations')\n",
    "    if not os.path.exists(annotation_folder_path): continue\n",
    "    annotation_map = {}\n",
    "    for json_file in os.listdir(annotation_folder_path):\n",
    "        json_path = os.path.join(annotation_folder_path, json_file)\n",
    "        try:\n",
    "            with open(json_path) as f: data = json.load(f)\n",
    "            asset_name = data['asset']['name']\n",
    "            annotation_map[asset_name] = json_path\n",
    "        except Exception: pass\n",
    "    for img_file in os.listdir(class_path):\n",
    "        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')) and img_file in annotation_map:\n",
    "            all_image_paths.append(os.path.join(class_path, img_file))\n",
    "            all_annotation_paths.append(annotation_map[img_file])\n",
    "\n",
    "print(\"Recriando a divisão de treino/teste...\")\n",
    "paths = list(zip(all_image_paths, all_annotation_paths))\n",
    "_, test_paths = train_test_split(paths, test_size=0.1, random_state=42)\n",
    "test_img_paths, test_ann_paths = zip(*test_paths)\n",
    "\n",
    "dataset_test = ArthropodDetectionDataset(\n",
    "    image_paths=list(test_img_paths),\n",
    "    annotation_paths=list(test_ann_paths),\n",
    "    transforms=get_transform(train=False)\n",
    ")\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "print(f\"Conjunto de teste carregado com {len(dataset_test)} imagens.\")\n",
    "\n",
    "CHECKPOINT_PATH = '/kaggle/input/modelo-6/modelo_epoca_6.pth'\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 8\n",
    "\n",
    "model = get_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"\\nCarregando checkpoint de '{CHECKPOINT_PATH}'...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Modelo da época 6 carregado com sucesso!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"ERRO: Checkpoint '{CHECKPOINT_PATH}' não encontrado. Verifique o caminho no painel 'Input'.\")\n",
    "\n",
    "def evaluate_model_refined(model, data_loader, device, score_threshold=0.5, iou_thresholds=np.arange(0.5, 1.0, 0.05)):\n",
    "    model.eval()\n",
    "    aps_per_threshold = {thr: [] for thr in iou_thresholds}\n",
    "    all_precisions, all_recalls, all_f1s = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Avaliando modelo refinado\", total=len(data_loader)):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs]\n",
    "            targets = [{k: v.to(\"cpu\") for k, v in t.items()} for t in targets]\n",
    "\n",
    "            for output, target in zip(outputs, targets):\n",
    "                gt_boxes = target[\"boxes\"]\n",
    "                scores = output[\"scores\"]\n",
    "                pred_boxes = output[\"boxes\"]\n",
    "\n",
    "                keep = scores >= score_threshold\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                scores = scores[keep]\n",
    "                \n",
    "                if len(pred_boxes) > 0:\n",
    "                    keep_idx = nms(pred_boxes, scores, 0.5)\n",
    "                    pred_boxes = pred_boxes[keep_idx]\n",
    "\n",
    "                if len(gt_boxes) == 0 and len(pred_boxes) == 0:\n",
    "                    continue\n",
    "\n",
    "                for thr in iou_thresholds:\n",
    "                    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
    "                         aps_per_threshold[thr].append(0)\n",
    "                         continue\n",
    "                    \n",
    "                    ious = box_iou(pred_boxes, gt_boxes)\n",
    "                    tp, fp = 0, 0\n",
    "                    matched_gt = set()\n",
    "\n",
    "                    if ious.numel() == 0:\n",
    "                        fp = len(pred_boxes)\n",
    "                    else:\n",
    "                        for i in range(len(pred_boxes)):\n",
    "                            iou_vals = ious[i]\n",
    "                            max_iou, max_idx = iou_vals.max(dim=0)\n",
    "                            if max_iou >= thr and max_idx.item() not in matched_gt:\n",
    "                                tp += 1\n",
    "                                matched_gt.add(max_idx.item())\n",
    "                            else:\n",
    "                                fp += 1\n",
    "                    \n",
    "                    fn = len(gt_boxes) - tp\n",
    "                    precision = tp / (tp + fp + 1e-6)\n",
    "                    recall = tp / (tp + fn + 1e-6)\n",
    "                    aps_per_threshold[thr].append(precision)\n",
    "    \n",
    "    mAP_50 = np.mean(aps_per_threshold.get(0.5, [0]))\n",
    "    mAP_50_95_list = [np.mean(v) for v in aps_per_threshold.values() if v]\n",
    "    mAP_50_95 = np.mean(mAP_50_95_list) if mAP_50_95_list else 0\n",
    "\n",
    "    mean_precision = np.mean([p for thr_aps in aps_per_threshold.values() for p in thr_aps])\n",
    "    \n",
    "    results = {\n",
    "        \"mAP@50\": mAP_50,\n",
    "        \"mAP@50:95\": mAP_50_95\n",
    "    }\n",
    "    return results\n",
    "\n",
    "refined_results = evaluate_model_refined(model, data_loader_test, device, score_threshold=0.5)\n",
    "\n",
    "print(\"\\n==================== RESULTADOS REFINADOS (Época 6) ====================\")\n",
    "print(f\"mAP@50:     {refined_results['mAP@50']:.4f}\")\n",
    "print(f\"mAP@50:95:  {refined_results['mAP@50:95']:.4f}\")\n",
    "print(\"=======================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94546d2d",
   "metadata": {},
   "source": [
    "## Exemplo do Modelo - Modelo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11edcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import v2 as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.patches as patches\n",
    "print(f\"Versão do PyTorch: {torch.__version__}\")\n",
    "print(f\"Versão do Torchvision: {torchvision.__version__}\")\n",
    "\n",
    "class SimpleImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms:\n",
    "            img, _ = self.transforms(img, None)\n",
    "        return img, img_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToImage())\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "DATASET_PATH = '/kaggle/input/arthropod-taxonomy-orders-object-detection-dataset/ArTaxOr'\n",
    "\n",
    "CHECKPOINT_PATH = '/kaggle/input/modelo-6/modelo_epoca_6.pth'\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 8\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 8\n",
    "\n",
    "all_image_paths = []\n",
    "for class_folder in sorted(os.listdir(DATASET_PATH)):\n",
    "    class_path = os.path.join(DATASET_PATH, class_folder)\n",
    "    if not os.path.isdir(class_path): continue\n",
    "    for img_file in os.listdir(class_path):\n",
    "        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            all_image_paths.append(os.path.join(class_path, img_file))\n",
    "\n",
    "_, test_img_paths = train_test_split(all_image_paths, test_size=0.1, random_state=42)\n",
    "\n",
    "model = get_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Carregando checkpoint de '{CHECKPOINT_PATH}'...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval() # Colocar o modelo em modo de avaliação!\n",
    "    print(\"Modelo carregado com sucesso!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"ERRO: Checkpoint '{CHECKPOINT_PATH}' não encontrado.\")\n",
    "\n",
    "class_map_inv = {1: 'Araneae', 2: 'Coleoptera', 3: 'Diptera', 4: 'Hemiptera', 5: 'Hymenoptera', 6: 'Lepidoptera', 7: 'Odonata'}\n",
    "\n",
    "random_image_path = random.choice(test_img_paths)\n",
    "print(f\"\\nImagem selecionada para detecção: {os.path.basename(random_image_path)}\")\n",
    "\n",
    "original_image = Image.open(random_image_path).convert(\"RGB\")\n",
    "transform = get_transform(train=False)\n",
    "image_tensor, _ = transform(original_image, None)\n",
    "image_tensor = image_tensor.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model([image_tensor])\n",
    "\n",
    "pred_boxes = prediction[0]['boxes'].cpu()\n",
    "pred_labels = prediction[0]['labels'].cpu()\n",
    "pred_scores = prediction[0]['scores'].cpu()\n",
    "\n",
    "score_threshold = 0.5\n",
    "keep = pred_scores > score_threshold\n",
    "final_boxes = pred_boxes[keep]\n",
    "final_labels = pred_labels[keep]\n",
    "final_scores = pred_scores[keep]\n",
    "\n",
    "labels_with_scores = [\n",
    "    f\"{class_map_inv.get(label.item(), 'Desconhecido')}: {score.item():.2f}\"\n",
    "    for label, score in zip(final_labels, final_scores)\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "ax.imshow(original_image)\n",
    "\n",
    "if len(final_boxes) > 0:\n",
    "    for box, label in zip(final_boxes, labels_with_scores):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        \n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), \n",
    "            width, \n",
    "            height, \n",
    "            linewidth=2, \n",
    "            edgecolor='blue', \n",
    "            facecolor='none'\n",
    "        )\n",
    "        \n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(\n",
    "            xmin,\n",
    "            ymin,\n",
    "            label,\n",
    "            color='white',\n",
    "            fontsize=12,\n",
    "            bbox=dict(facecolor='blue', alpha=0.6, pad=1),\n",
    "            verticalalignment='bottom'\n",
    "        )\n",
    "else:\n",
    "    print(\"Nenhum artrópode detectado com confiança >\", score_threshold)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.title(f\"Detecções em: {os.path.basename(random_image_path)}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
